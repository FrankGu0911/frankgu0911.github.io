

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.ico">
  <link rel="icon" href="/images/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Frank Gu">
  <meta name="keywords" content="">
  
    <meta name="description" content="引言在自动驾驶领域，基于模仿学习的端到端模型因其简单的结构与复杂场景下强大的泛化性能而受到关注。受限于其原理，模仿学习的输出往往难以达到或超越直接模仿的专家系统的水平。因此，模仿学习的数据集的质量对模型的性能有着至关重要的影响。目前被广泛使用的专家数据包括基于规则的专家算法采集的数据、基于强化学习的专家算法采集的数据以及人类驾驶专家采集的数据。基于规则的专家算法则需详尽定义各类规则，违背了端到端模">
<meta property="og:type" content="article">
<meta property="og:title" content="基于多模态语言模型的低延迟端到端自动驾驶模型">
<meta property="og:url" content="https://frankgu0911.github.io/2024/05/03/paper2/index.html">
<meta property="og:site_name" content="Frank Tech Blog">
<meta property="og:description" content="引言在自动驾驶领域，基于模仿学习的端到端模型因其简单的结构与复杂场景下强大的泛化性能而受到关注。受限于其原理，模仿学习的输出往往难以达到或超越直接模仿的专家系统的水平。因此，模仿学习的数据集的质量对模型的性能有着至关重要的影响。目前被广泛使用的专家数据包括基于规则的专家算法采集的数据、基于强化学习的专家算法采集的数据以及人类驾驶专家采集的数据。基于规则的专家算法则需详尽定义各类规则，违背了端到端模">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://frankgu0911.github.io/images/paper2/%E5%9B%BE1.png">
<meta property="og:image" content="https://frankgu0911.github.io/images/paper2/%E5%9B%BE2.png">
<meta property="og:image" content="https://frankgu0911.github.io/images/paper2/%E5%9B%BE3.png">
<meta property="article:published_time" content="2024-05-03T13:29:29.000Z">
<meta property="article:modified_time" content="2024-05-03T14:10:37.757Z">
<meta property="article:author" content="Frank Gu">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://frankgu0911.github.io/images/paper2/%E5%9B%BE1.png">
  
  
  
  <title>基于多模态语言模型的低延迟端到端自动驾驶模型 - Frank Tech Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"frankgu0911.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Frank Tech Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/banner/banner-2024-1.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.6)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="基于多模态语言模型的低延迟端到端自动驾驶模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-03 21:29" pubdate>
          2024年5月3日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          50 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">基于多模态语言模型的低延迟端到端自动驾驶模型</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在自动驾驶领域，基于模仿学习的端到端模型因其简单的结构与复杂场景下强大的泛化性能而受到关注。受限于其原理，模仿学习的输出往往难以达到或超越直接模仿的专家系统的水平。因此，模仿学习的数据集的质量对模型的性能有着至关重要的影响。目前被广泛使用的专家数据包括基于规则的专家算法采集的数据、基于强化学习的专家算法采集的数据以及人类驾驶专家采集的数据。基于规则的专家算法则需详尽定义各类规则，违背了端到端模型简洁的设计初衷，且限制了端到端算法在复杂场景下的泛化性能；基于强化学习的专家算法虽然在决策能力上表现出色，但其生成的策略与人类驾驶习惯差异显著；人类专家的驾驶数据进行学习可以提高模型的实际应用价值，但这种方法通常缺乏决策过程的可解释性，且人类行为存在个体差异，决策具有一定的不一致性，这些原因限制了其广泛应用。近年来，随着多模态大型语言模型的发展，通用人工智能领域取得了显著进展。这些模型在包括自动驾驶在内的多个领域展现出优秀的性能，并且具备较高的可解释性。然而，大语言模型在实时推理任务中表现出较慢的处理速度和较低的实时性，因此在需要即时响应的应用场景中难以直接使用。<br>鉴于此，本研究针对基于模仿学习的端到端算法中使用的专家数据集，在数据标注与决策算法上进行创新。本文的主要贡献如下：</p>
<ol>
<li>提出了一种利用多模态大语言模型对自动驾驶数据集进行决策原因和关键物体的标注方法，以提高人类驾驶数据集的可解释性；</li>
<li>针对端到端模型的可解释性低的问题，采用多任务学习的方式输出决策原因与关键目标，在提高性能的同时，提供了十分直观的决策解释；</li>
<li>为解决大语言模型在自动驾驶中直接推理时实时性较低的问题，提出了在数据集处理中使用大语言模型对驾驶数据进行详尽的标注，避免了实时推理阶段的延迟，从而在保证实时性的情况下增加了可解释性。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>随着现代人工智能技术的快速发展，端到端学习方法已广泛应用于汽车自动驾驶技术中。与难以收敛且与人类驾驶习惯有显著差异的强化学习（Reinforcement Learning, RL）相比，模仿学习（Imitation Learning, IL）[1,2]通过直接利用专家数据进行监督学习，不需要像强化学习那样从头开始探索。这种方法不仅收敛速度快，而且可以获得较为稳定和可预测的行为输出。</p>
<p>随着自动驾驶模仿学习的发展，方法从最初的直接模仿动作，逐步过渡到更复杂的轨迹模仿和多任务学习。文献（CIL）使用自车元信息、高级指令与图像编码器，对控制动作进行直接预测。在此基础上，文献（CILRS）随后被提出，进一步引入了目标速度预测。然而，这种基于直接控制的方法通常具有更高的车辆碰撞率。文献Transfuser及其变体使用一个简单的GRU来自回归预测行驶路径点，有效降低了碰撞率。文献LAV在此基础上采用时序GRU模块来进一步细化轨迹。这些基于轨迹的方法都使用PID控制器来获得最终动作，这使得在复杂情景中可能导致效果不佳。文献TCP提出了一种融合控制动作与路径规划的方法，结合两种输出形式的优点，进一步提升在不同场景下的表现。文献Interfuser则通过多任务学习将安全相关输出与路径规划输出分离，进一步提升了模型的性能同时，提高了模型的可解释性。文献LCDiff则进一步引入了BEV预测模块，提升了模型在复杂场景下的泛化性能。</p>
<p>在模仿学习中，选择正确的数据集对于算法的性能和可行性至关重要。文献（TCP，Roach）使用了具有全局信息的强化学习专家行驶数据作为数据集进行训练，展示了在处理复杂交通环境时的高效性能，然而其生成的策略与人类驾驶习惯差异显著，在一些复杂情况下通常会做出一些不可解释的危险行为。<br>文献（Interfuser，Transfuser，TF++WP）使用了规则专家的驾驶数据作为数据集进行训练，证明了这种方法在保持行为一致性和规则遵循方面的有效性。尽管如此，这种方法通常缺乏应对未预见情况的灵活性。使用人类专家的驾驶数据进行学习可以提高模型的实际应用价值，但人类行为的个体差异和决策不一致性使得作为数据集时，模型训练出来的结果往往缺乏可解释性。</p>
<p>近年来多模态大语言模型等通用人工智能技术发展迅速，研究者开始探索将这些先进模型应用于自动驾驶的可能性。文献LMDrive使用自然语言指令作为输入，结合多传感器数据，通过多模态大语言模型生成驾驶决策，在输出轨迹与控制动作的同时，提供了决策原因的解释，增强了模型的可解释性和透明度。然而，大语言模型在实时推理任务中表现出较慢的处理速度和较低的实时性，在真实驾驶场景中难以直接使用。文献DriveVLM提出了一种双系统快慢机制，通过使用传统的端到端模型进行快速推理，再通过大语言模型进行慢速推理，一定程度上缓解了实时性问题，提高了系统的响应速度和实用性。尽管有所改进，这种双系统机制在处理高度动态和复杂的驾驶场景时仍显示出性能和实时性的局限，特别是在需要同时考虑多个突发因素和紧急响应的情况下。</p>
<h1 id="方法与实现"><a href="#方法与实现" class="headerlink" title="方法与实现"></a>方法与实现</h1><p>本文提出了一种新颖的基于多任务学习的端到端自动驾驶模型，并结合了多模态大语言模型来标注人类驾驶数据集中的决策原因和关键物体，以提高模型的性能和可解释性。本方法主要包括多模态大语言模型标注与多任务模仿学习两个部分。如图1所示，本模型在训练前，先通过多模态大语言模型对人类驾驶数据集进行全面的标注。针对包含刹车、大幅转向等关键驾驶行为的数据帧X_t，使用X_(t-3)到X_t的连续帧中的图像、雷达、交通参与者信息与控制作为输入，得出驾驶意图I_t与关键物体O_t的标注。在训练与推理阶段，将多模态信息输入感知编码器得到融合特征$F_t$，将$F_t$输入到决策模型中，在预测航点$W_t$-$W_{t+3}$的同时，预测所需驾驶意图$I_t$与关键物体$O_t$，以多任务输出的方式提高模型的性能与可解释性。通过预测的航点$W_t$-$W_{t+3}$与当前速度$V_t$、当前方向角$\theta_t$，分别对速度与方向角进行PID控制，得出最终的控制动作预测：转向$\delta_t$、油门$T_t$与刹车$B_t$。<br><img src="/images/paper2/%E5%9B%BE1.png" srcset="/img/loading.gif" lazyload alt="图1. 模型结构"></p>
<h2 id="自动驾驶模型建模"><a href="#自动驾驶模型建模" class="headerlink" title="自动驾驶模型建模"></a>自动驾驶模型建模</h2><p>自动驾驶模型的目标是在复杂的城市环境中，在各种交通状况、道路类型和不可预测元素（如行人过街和各类天气）的情况下，使车辆能够自主、安全、高效地行驶。模型从起点A行驶至终点B，通过全球导航卫星系统（GNSS）坐标$TP_0$至$TP_n$指引路径。在行驶中的每个时刻t，车辆可以获得当前位置$P_t$、当前速度$V_t$、当前方向角$\theta_t$，并根据不同的传感器配置，可以获得图像数据$I_t$、激光雷达数据$L_t$等多模态信息。基于这些输入，端到端自动驾驶模型$Model(.)$的任务是输出车辆的控制动作，基础模型设计如下：<br>$$\delta_t, T_t, B_t &#x3D; Model(TP_{0-n}, P_{0-t}, V_{0-t}, \theta_{0-t}, I_{0-t}, L_{0-t})$$<br>其中，$\delta_t$、$T_t$、$B_t$分别表示车辆在时刻t的转向、油门和刹车控制动作。$Model(.)$表示一个端到端的自动驾驶模型，输入为导航点、车辆位置、速度、方向角、图像和激光雷达数据，输出为车辆的控制动作。自动驾驶模型的任务就是通过处理这些多模态输入，解读复杂的环境信息，并在不违反交通规则的前提下，做出合理的决策，以实现车辆的自主行驶。</p>
<p>此外，为了使模型能够充分理解驾驶环境，本研究设计了一个多任务学习框架，以提高模型的性能和可解释性。驾驶意图$I_t$与关键物体$O_t$被设定为多任务学习的目标，同时环境感知任务，例如BEV特征$BEV_t$，也被列为多任务学习目标。总体的模型设计如下：<br>$$\delta_t, T_t, B_t, I_t, O_t, BEV_t &#x3D; Model(TP_{0-n}, P_{0-t}, V_{0-t}, \theta_{0-t}, I_{0-t}, L_{0-t})$$</p>
<h2 id="数据采集与多模态大语言模型标注"><a href="#数据采集与多模态大语言模型标注" class="headerlink" title="数据采集与多模态大语言模型标注"></a>数据采集与多模态大语言模型标注</h2><p>为了支撑这些多任务学习目标，本研究需要一个精确标注的数据集。因此，在端到端自动驾驶领域被广泛采用的CARLA[]模拟器被选为数据采集工具，该模拟器能够提供一个逼真的动态闭环世界。数据收集包括两个阶段：</p>
<ol>
<li>人类专家在CARLA中根据导航点驾驶，并收集传感器数据和控制信号；</li>
<li>使用多模态大语言模型对包含刹车、大幅转向等关键驾驶行为的数据进行驾驶意图与关键物体的标注。</li>
</ol>
<p>人类专家在CARLA模拟器中进行人工驾驶，并以2Hz的频率进行数据采集，共采集了2.2万帧的驾驶数据。每一帧数据包括了车辆的当前位置、速度、方向角、导航点、图像和激光雷达的传感器数据，油门、刹车与转向的控制动作，以及附近交通参与者位置与BEV特征的高级信息。为了增强收集数据集的多样性，采集过程共使用了14种天气，涵盖了早晨、中午、傍晚等不同时间段，以及晴天、阴天、雨天等不同天气条件。传感器配置上使用了三个RGB摄像头（左、前、右）和一个激光雷达，侧面摄像头的角度为60°。为了验证模型的泛化能力，LAV[]路线中的Town02与Town05在采集过程中被有意避免，并被设置为测试场景。</p>
<p>本研究通过对数据集进行详细标注来提高模型的性能和解释性。针对包含刹车、大幅转向等关键驾驶行为的数据帧，需要对其进行驾驶意图与关键物体的标注。为了实现这一目标，本研究使用了多模态大语言模型对数据集进行标注。整体流程如图2所示，首先将连续的四帧的的图像序列作为输入，并通过文本的方式输入当前帧的控制信息，得到驾驶意图输出与其对应描述。驾驶意图被分为17类，包括但不限于加速、减速、转向、变道、绕行、轻微位置调整和等待。在得到驾驶意图后，继续通过多模态大语言模型的上下文记忆能力，标注出影响决策的关键物体。关键物体被定义为在当前帧中对驾驶意图产生重要影响的物体，如前方车辆、行人、交通标志、信号灯等。由于大语言模型对数值计算的支持较弱，本研究通过输入当前帧的交通参与者相对位置信息的方式，让大语言模型通过选择而非计算来标注出关键物体。<br><img src="/images/paper2/%E5%9B%BE2.png" srcset="/img/loading.gif" lazyload alt="图2. 标注"></p>
<h2 id="多任务学习框架"><a href="#多任务学习框架" class="headerlink" title="多任务学习框架"></a>多任务学习框架</h2><p>为了提高模型的性能和可解释性的同时，保证模型的实时性，本研究设计了一个多任务学习框架，该框架针对由大语言模型完成标注的驾驶意图与关键物体进行学习。主任务设置为直接影响驾驶行为的航点预测任务，而辅助任务包括环境感知任务、驾驶意图预测以及关键物体预测。这些辅助任务旨在促进模型对驾驶环境的理解，从而提高模型的性能，同时提供直观的决策解释。</p>
<p>如图3所示，多任务学习的目标包括环境感知任务、航点预测、驾驶意图预测和关键物体预测。为了避免[TF++]中提出的捷径问题，并确保车辆始终保持在车道线内，该框架使用了一个标准的Transformer解码器来处理感知特征$F_t$。一部分经处理的嵌入特征被输入到门控循环网络（GRU）中，以预测未来航点的位置$W_t$-$W_{t+3}$。与此同时，另一部分嵌入特征被输入到安全解释器中，用于预测驾驶意图$I_t$与关键物体$O_t$。在感知任务中，感知解码器根据感知特征$F_t$对BEV特征$BEV_t$进行预测。</p>
<p>预测的航点$W_t$到$W_{t+3}$，结合当前速度$V_t$和方向角$\theta_t$，被进一步处理以输出对车辆的控制值。这些数据输入到两个独立的PID控制器，分别调节速度和方向，输出最终的控制动作预测：转向$\delta_t$、油门$T_t$与刹车$B_t$。<br><img src="/images/paper2/%E5%9B%BE3.png" srcset="/img/loading.gif" lazyload alt="图3. 多任务学习"></p>
<!-- ## 实施细节 -->

<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>在自动驾驶任务中，由于对模型的评估不能依赖简单的准确率或偏离率指标，因此，本研究采用了Carla模拟器来进行闭环模拟驾驶。为了测试自动驾驶系统的稳健性和反应能力，设计了一系列复杂的城市环境，包括多车道道路、复杂的路口、以及偶尔出现的突发交通状况。模型的评估采用了几个关键的指标：路线完成度（Route Completion，RC），该指标衡量车辆成功完成路线的百分比；违规分数（Infraction Score，IS），该指标随着车辆违反交通规则的次数增加而降低；综合驾驶分数（Driving Score，DS），即RC与IS的乘积，用于综合评价模型的性能。这些指标涵盖了模型完成路线的能力与遵守交通规则的表现，是Carla闭环模拟中评价自动驾驶模型性能的重要指标。</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>实验环境使用0.9.10.1版本的Carla模拟器，并使用Carla leaderboard来评估模型的性能。模型在两组不同的路线与14种天气条件上进行测试，包括Longest6[]路线与LAV[]路线。其中Longest6路线所涉及的城镇在训练时被使用，而LAV路线的城镇则未在训练中使用，以验证模型的泛化能力。</p>
<p>感知编码器上，我们分别使用了LCDiff所提出的BEV感知模型和Transfuser的感知模块，以验证本研究提出的多任务学习框架的性能。LCDiff使用了扩散模型，并使用LiDar数据通过ControlNet结构来引导BEV的生成，从而能够输出高精度的BEV特征。Transfuser的感知模块使用Transformer在不同尺度上对图像与LiDar进行融合，并采用自注意力机制来增强全局环境理解。</p>
<h2 id="模型性能对比"><a href="#模型性能对比" class="headerlink" title="模型性能对比"></a>模型性能对比</h2><p>在LCDiff和Transfuser外，本研究的多任务学习框架还与其他几个最先进的自动驾驶模型进行了比较，包括LAV与Interfuser。LAV通过使用模型能观测到的所有车辆的数据来进行数据增强，从而提高了模型的泛化能力。Interfuser通过多任务学习将安全相关输出与路径规划输出分离，进一步提升了模型的性能。值得注意的是，被比较的模型均使用了规则专家或强化学习专家的驾驶数据集进行训练，而本研究的模型则使用了人类专家的驾驶数据集经过多模态大语言模型标注后进行训练。</p>
<p>表1. 在Longest6路线上的实验结果*</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>DS↑</th>
<th>RC↑</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>LAV</td>
<td>58.3±1.1</td>
<td>83.2±1.3</td>
<td>0.68±0.02</td>
</tr>
<tr>
<td>Interfuser</td>
<td>46.8±5.6</td>
<td>74.3±1.0</td>
<td>0.63±0.07</td>
</tr>
<tr>
<td>Transfuser</td>
<td>46.6±6.2</td>
<td>92.9±1.2</td>
<td>0.50±0.06</td>
</tr>
<tr>
<td>LCDiff</td>
<td>53.3±2.6</td>
<td>94.9±0.7</td>
<td>0.56±0.02</td>
</tr>
<tr>
<td>Ours(Transfuer-based)</td>
<td>52.7±2.7</td>
<td>95.6±1.5</td>
<td>0.55±0.02</td>
</tr>
<tr>
<td>Ours(LCDiff-based)</td>
<td>64.3±2.5</td>
<td>97.6±0.9</td>
<td>0.66±0.02</td>
</tr>
</tbody></table>
<p>*3次实验的平均值与标准差</p>
<p>从表1中可以看出，本研究提出的多任务学习框架在训练城镇Longest6路线上的表现优于其他几个最先进的自动驾驶模型。以Transfuser为感知模块的多任务学习框架在DS、RC和IS上比原模型提高了13.1%、2.9%和10.0%，而以LCDiff为感知模块的多任务学习框架在DS、RC和IS上则比原模型提高了14.8%、2.8%和12.1%。这表明本研究提出的辅助任务驾驶意图预测与关键物体预测，能够有效提高模型遵守交通规则的能力，同时也增强了模型在复杂环境下的导航准确性。在使用LCDiff感知模块的框架中，得益于高精度BEV特征作为辅助任务，模型在IS上取得了很大的提升。</p>
<p>表2. 在LAV路线上的实验结果*</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>DS↑</th>
<th>RC↑</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>LAV</td>
<td>60.4±2.5</td>
<td>94.5±1.1</td>
<td>0.64±0.02</td>
</tr>
<tr>
<td>Interfuser</td>
<td>38.2±1.5</td>
<td>62.7±3.6</td>
<td>0.61±0.04</td>
</tr>
<tr>
<td>Transfuser</td>
<td>39.1±9.2</td>
<td>84.2±6.8</td>
<td>0.46±0.06</td>
</tr>
<tr>
<td>LCDiff</td>
<td>55.6±3.0</td>
<td>98.1±1.0</td>
<td>0.57±0.03</td>
</tr>
<tr>
<td>Ours(Transfuer-based)</td>
<td>50.3±2.2</td>
<td>85.9±2.0</td>
<td>0.59±0.02</td>
</tr>
<tr>
<td>Ours(LCDiff-based)</td>
<td>63.9±3.1</td>
<td>98.4±0.6</td>
<td>0.65±0.03</td>
</tr>
</tbody></table>
<p>*3次实验的平均值与标准差</p>
<p>从表2中可以看出，本研究提出的多任务学习框架在未见过的验证城镇LAV路线上同样表现出色。基于Transfuser的变体在DS、RC和IS上比原模型提高了28.6%、2.0%和28.3%，而基于LCDiff的变体在DS、RC和IS上则比原模型提高了14.9%、0.4%和14.0%。由于LAV路线的城镇未在训练中使用，Transfuser的基础模型在IS上表现较差，比较容易违反交通规则，而本研究提出的多任务学习框架使其在IS上取得了很大的提升。在使用LCDiff感知模块的框架中，由于其泛化能力较Transfuser更强，基础模型已有较高的RC，然而在IS上，多任务学习框架的引入仍然使其取得了进一步的提升。</p>
<p>针对自动驾驶模型的实时性，本研究在不同模型上进行了推理时间的对比。实验环境中，CPU为AMD Ryzen 9 7950x， GPU为NVIDIA RTX 3090，内存为64GB，实验系统为Ubuntu 22.04。实验路线选用了Longest6路线，实验取每一帧的平均推理时间作为对比指标。除了Transfuser和LCDiff外，LMDrive[]作为直接使用多模态大语言模型的代表也被选为对比对象，使用了其提供的基于LLaVA-v1.5-7B微调的模型。实验结果如表3所示。</p>
<p>表3. 单帧平均推理时长</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>推理时长(ms)↓</th>
<th>DS↑</th>
<th>RC↑</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>Transfuser</td>
<td>44ms</td>
<td>46.6</td>
<td>92.9</td>
<td>0.50</td>
</tr>
<tr>
<td>LCDiff</td>
<td>159ms</td>
<td>53.3</td>
<td>94.9</td>
<td>0.56</td>
</tr>
<tr>
<td>LMDrive</td>
<td>1769ms</td>
<td>44.5</td>
<td>51.1</td>
<td>0.87</td>
</tr>
<tr>
<td>Ours(Transfuer-based)</td>
<td>47ms</td>
<td>52.7</td>
<td>95.6</td>
<td>0.55</td>
</tr>
<tr>
<td>Ours(LCDiff-based)</td>
<td>166ms</td>
<td>64.3</td>
<td>97.6</td>
<td>0.66</td>
</tr>
</tbody></table>
<p>从表3中可以看出，本研究提出的多任务学习框架在推理时间上对比基础模型Transfuser和LCDiff相当，且在性能上均有所提升。LCDiff的推理时间较长，主要是因为高精度BEV特征的生成与其扩散模型的架构限制。对比起直接使用多模态大语言模型的LMDrive，多任务学习框架在推理时间上有较大的优势。值得注意的是，LMDrive在路线完成度（RC）上表现不佳，主要原因在于该模型使用大语言模型直接进行数值预测，这种方法在处理具体的导航任务时可能不如基于回归的方法精确。然而，在违规分数IS上，LMDrive的表现较好，其遵守交通规则的能力明显优于其他模型，这表明大语言模型在预见潜在违规行为方面具有很大的优势。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>针对基于模仿学习的端到端自动驾驶模型，数据集的选择对模型训练和性能具有重要影响。为了深入研究这一影响，本研究进行了消融实验，通过比较规则专家数据集与人类专家数据集对模型的影响，探讨了这些数据集在模仿学习中的作用及其对模型性能的具体影响。针对规则专家数据集，其标签中的驾驶意图与关键物体是由专家算法经过分支判断后得出的，而人类专家数据集则是由人类专家驾驶员在CARLA模拟器中进行人工驾驶后得到的，其驾驶意图与关键物体的标注则是通过多模态大语言模型完成的。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>数据集</th>
<th>多任务学习</th>
<th>DS↑</th>
<th>RC↑</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>Our(Transfuer-based)</td>
<td>规则专家</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>人类专家</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>规则专家</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>人类专家</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>规则专家</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>人类专家</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>规则专家</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>人类专家</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>从表4中可以看出，无论是基于Transfuser还是LCDiff的多任务学习框架，使用人类专家数据集进行训练均优于使用规则专家数据集。这表明人类专家数据集在模仿学习中具有更高的价值。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>驾驶意图</th>
<th>关键物体</th>
<th>DS↑</th>
<th>RC↑</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>Our(Transfuer-based)</td>
<td>×</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>×</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>√</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(Transfuer-based)</td>
<td>√</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>×</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>×</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>√</td>
<td>×</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Our(LCDiff-based)</td>
<td>√</td>
<td>√</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] POMERLEAU D A.ALVINN:an autonomous land vehicle in a neural network[C]&#x2F;Proceedings of the 1st International Conference on Neural Information Processing Systems.Cambridge,MA,USA:[s.n.],1988:305-313.</p>
<p>[2] BOJARSKI M,DELTESTA D,DWORAKOWSKI D,et al.End to End Learning for Self-Driving Cars[EB&#x2F;OL].(2016-04-25) [2022-12-20].<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1604.07316">http://arxiv.org/abs/1604.07316</a>.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/17/Quest-3D-video/" title="如何使用Quest3录制3D视频">
                        <span class="hidden-mobile">如何使用Quest3录制3D视频</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
